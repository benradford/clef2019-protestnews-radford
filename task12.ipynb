{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "import os\n",
    "import gc \n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\";  \n",
    "\n",
    "import glob\n",
    "import datetime\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import math\n",
    "from collections import Counter\n",
    "from ftfy import fix_text\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.wrappers import FastText \n",
    "\n",
    "from keras.preprocessing.sequence import make_sampling_table, skipgrams\n",
    "from keras.layers import Input, Dense, Embedding, Activation, Dot, Flatten, GRU, Dropout, LSTM, Bidirectional, GlobalMaxPooling1D, Concatenate, Lambda, CuDNNGRU, CuDNNLSTM, GaussianNoise, GaussianDropout, Conv1D, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import load_model\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "fasttext = FastText.load_fasttext_format(\"wiki.en.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "entity_map = {\"PERSON\":\"person\",\n",
    "              \"NORP\":\"nationality\",\n",
    "              \"FAC\":\"facility\",\n",
    "              \"ORG\":\"organization\",\n",
    "              \"GPE\":\"government\",\n",
    "              \"LOC\":\"location\",\n",
    "              \"PRODUCT\":\"product\",\n",
    "              \"EVENT\":\"event\",\n",
    "              \"WORK_OF_ART\":\"artwork\",\n",
    "              \"LAW\":\"law\",\n",
    "              \"LANGUAGE\":\"language\",\n",
    "              \"DATE\":\"date\",\n",
    "              \"TIME\":\"time\",\n",
    "              \"PERCENT\":\"percent\",\n",
    "              \"MONEY\":\"money\",\n",
    "              \"QUANTITY\":\"amount\",\n",
    "              \"ORDINAL\":\"first\",\n",
    "              \"CARDINAL\":\"number\"}\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub(' ', text)\n",
    "\n",
    "def strip_whitespace(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "def clean_text(text):\n",
    "    return(remove_punctuation(fix_text(strip_whitespace(remove_tags(text)))).lower())\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return(re.sub(r\"[^A-Za-z0-9 ]\",\"\",text))\n",
    "\n",
    "def sub_entities(text, spacy_model):\n",
    "#     replacements = [(e.text,e.label_) for e in spacy_model(text).ents]\n",
    "#     replacements = sorted(replacements, key=lambda x: len(x[0]), reverse=True)\n",
    "#     replacements = [(a,entity_map[b]) for a,b in replacements]\n",
    "#     for k,v in replacements:\n",
    "#         text = re.sub(r\"\\b\"+re.escape(k)+r\"\\b\", v, text)\n",
    "# #     text = re.sub(r\"[^a-zA-Z0-9 ]\",\"\",text).lower()\n",
    "    return(text)\n",
    "\n",
    "def df_to_dic(df):\n",
    "    data_dic = {}\n",
    "    for item in df.iterrows():\n",
    "        data = item[1]\n",
    "        _id = data[\"id\"]\n",
    "        data_dic[_id] = {}\n",
    "        data_dic[_id][\"label\"] = data[\"label\"]\n",
    "        data_dic[_id][\"text\"] = data[\"text\"]\n",
    "        data_dic[_id][\"url\"] = data[\"url\"]\n",
    "    return data_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 5\n",
    "window_size = 5\n",
    "negative_samples = 10\n",
    "dim = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## News Data\n",
    "\n",
    "file_name_train_task1 = \"data/Document/train_filled.json\"\n",
    "file_name_dev_task1 = \"data/Document/dev_filled.json\"\n",
    "file_name_test_task1 = \"data/Document/test_filled.json\"\n",
    "file_name_china_task1 = \"data/Document/test_china_filled.json\"\n",
    "\n",
    "file_name_train_task2 = \"data/Sentence/train_filled.json\"\n",
    "file_name_dev_task2 = \"data/Sentence/dev_filled.json\"\n",
    "file_name_test_task2 = \"data/Sentence/test_filled.json\"\n",
    "file_name_china_task2 = \"data/Sentence/test_china_filled.json\"\n",
    "\n",
    "## JSON - to - DF\n",
    "\n",
    "df_train_task1 = pd.read_json(file_name_train_task1, orient=\"records\", lines = True)\n",
    "df_train_task1[\"text\"] = [sub_entities(clean_text(t), nlp) for t in df_train_task1.text]\n",
    "\n",
    "df_dev_task1 = pd.read_json(file_name_dev_task1, orient=\"records\", lines = True)\n",
    "df_dev_task1[\"text\"] = [sub_entities(clean_text(t), nlp) for t in df_dev_task1.text]\n",
    "\n",
    "df_test_task1 = pd.read_json(file_name_test_task1, orient=\"records\", lines = True)\n",
    "df_test_task1[\"text\"] = [sub_entities(clean_text(t), nlp) for t in df_test_task1.text]\n",
    "df_test_task1[\"label\"] = -1\n",
    "\n",
    "df_train_task2 = pd.read_json(file_name_train_task2, orient=\"records\", lines = True)\n",
    "df_train_task2[\"text\"] = [sub_entities(clean_text(t), nlp) for t in df_train_task2.sentence]\n",
    "\n",
    "df_dev_task2 = pd.read_json(file_name_dev_task2, orient=\"records\", lines = True)\n",
    "df_dev_task2[\"text\"] = [sub_entities(clean_text(t), nlp) for t in df_dev_task2.sentence]\n",
    "\n",
    "df_test_task2 = pd.read_json(file_name_test_task2, orient=\"records\", lines = True)\n",
    "df_test_task2[\"text\"] = [sub_entities(clean_text(t), nlp) for t in df_test_task2.sentence]\n",
    "df_test_task2[\"label\"] = -1\n",
    "\n",
    "df_china_task1 = pd.read_json(file_name_china_task1, orient=\"records\", lines = True)\n",
    "df_china_task1[\"text\"] = [sub_entities(clean_text(t), nlp) for t in df_china_task1.text]\n",
    "df_china_task1[\"label\"] = -1\n",
    "\n",
    "df_china_task2 = pd.read_json(file_name_china_task2, orient=\"records\", lines = True)\n",
    "df_china_task2[\"text\"] = [sub_entities(clean_text(t), nlp) for t in df_china_task2.sentence]\n",
    "df_china_task2[\"label\"] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_china_task1.head())\n",
    "print(df_china_task2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Max len task1: {max([len(x.split()) for x in df_train_task1.text.tolist()])}\")\n",
    "print(f\"Min len task1: {min([len(x.split()) for x in df_train_task1.text.tolist()])}\")\n",
    "print(f\"Min len task1: {sum([len(x.split()) for x in df_train_task1.text.tolist()])/len(df_train_task1.text.tolist())}\")\n",
    "\n",
    "print(f\"Max len task2: {max([len(x.split()) for x in df_train_task2.text.tolist()])}\")\n",
    "print(f\"Min len task2: {min([len(x.split()) for x in df_train_task2.text.tolist() if len(x.split())>0])}\")\n",
    "print(f\"Min len task2: {sum([len(x.split()) for x in df_train_task2.text.tolist()])/len(df_train_task2.text.tolist())}\")\n",
    "\n",
    "df_train_task2.loc[df_train_task2.sentence.apply(len)<15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_pass(text, model):\n",
    "    try:\n",
    "        return(model[text])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataframe to Dictionary\n",
    "\n",
    "dict_train_task1 = df_to_dic(df_train_task1)\n",
    "dict_test_task1 = df_to_dic(df_test_task1)\n",
    "dict_dev_task1 = df_to_dic(df_dev_task1)\n",
    "dict_china_task1 = df_to_dic(df_china_task1)\n",
    "\n",
    "dict_train_task2 = df_to_dic(df_train_task2)\n",
    "dict_test_task2 = df_to_dic(df_test_task2)\n",
    "dict_dev_task2 = df_to_dic(df_dev_task2)\n",
    "dict_china_task2 = df_to_dic(df_china_task2)\n",
    "\n",
    "## Sub in Word Vectors\n",
    "\n",
    "for id_, dic in dict_train_task1.items():\n",
    "    dict_train_task1[id_][\"tokenized\"] = [try_pass(a,fasttext) for a in dic[\"text\"].split()]# if a in fasttext.vocab.keys()]\n",
    "    dict_train_task1[id_][\"tokenized\"] = [a for a in dict_train_task1[id_][\"tokenized\"] if a is not None]\n",
    "for id_, dic in dict_test_task1.items(): \n",
    "    dict_test_task1[id_][\"tokenized\"] = [try_pass(a,fasttext) for a in dic[\"text\"].split()]# if a in fasttext.vocab.keys()]\n",
    "    dict_test_task1[id_][\"tokenized\"] = [a for a in dict_test_task1[id_][\"tokenized\"] if a is not None]\n",
    "for id_, dic in dict_dev_task1.items():\n",
    "    dict_dev_task1[id_][\"tokenized\"] = [try_pass(a,fasttext) for a in dic[\"text\"].split()]# if a in fasttext.vocab.keys()]\n",
    "    dict_dev_task1[id_][\"tokenized\"] = [a for a in dict_dev_task1[id_][\"tokenized\"] if a is not None]\n",
    "for id_, dic in dict_china_task1.items():\n",
    "    dict_china_task1[id_][\"tokenized\"] = [try_pass(a,fasttext) for a in dic[\"text\"].split()]# if a in fasttext.vocab.keys()]\n",
    "    dict_china_task1[id_][\"tokenized\"] = [a for a in dict_china_task1[id_][\"tokenized\"] if a is not None]\n",
    "    \n",
    "for id_, dic in dict_train_task2.items():\n",
    "    dict_train_task2[id_][\"tokenized\"] = [try_pass(a,fasttext) for a in dic[\"text\"].split()]# if a in fasttext.vocab.keys()]\n",
    "    dict_train_task2[id_][\"tokenized\"] = [a for a in dict_train_task2[id_][\"tokenized\"] if a is not None]\n",
    "for id_, dic in dict_test_task2.items(): \n",
    "    dict_test_task2[id_][\"tokenized\"] = [try_pass(a,fasttext) for a in dic[\"text\"].split()]# if a in fasttext.vocab.keys()]\n",
    "    dict_test_task2[id_][\"tokenized\"] = [a for a in dict_test_task2[id_][\"tokenized\"] if a is not None]\n",
    "for id_, dic in dict_dev_task2.items():\n",
    "    dict_dev_task2[id_][\"tokenized\"] = [try_pass(a,fasttext) for a in dic[\"text\"].split()]# if a in fasttext.vocab.keys()]\n",
    "    dict_dev_task2[id_][\"tokenized\"] = [a for a in dict_dev_task2[id_][\"tokenized\"] if a is not None]\n",
    "for id_, dic in dict_china_task2.items():\n",
    "    dict_china_task2[id_][\"tokenized\"] = [try_pass(a,fasttext) for a in dic[\"text\"].split()]# if a in fasttext.vocab.keys()]\n",
    "    dict_china_task2[id_][\"tokenized\"] = [a for a in dict_china_task2[id_][\"tokenized\"] if a is not None]\n",
    "                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_x_train_task1 = [v[\"tokenized\"] for k,v in dict_train_task1.items()]\n",
    "seq_y_train_task1 = np.array([v[\"label\"] for k,v in dict_train_task1.items()])\n",
    "max_len_train_task1 = max([len(x) for x in seq_x_train_task1])\n",
    "\n",
    "seq_x_dev_task1 = [v[\"tokenized\"] for k,v in dict_dev_task1.items()]\n",
    "seq_y_dev_task1 = np.array([v[\"label\"] for k,v in dict_dev_task1.items()])\n",
    "max_len_dev_task1 = max([len(x) for x in seq_x_dev_task1])\n",
    "\n",
    "seq_x_test_task1 = [v[\"tokenized\"] for k,v in dict_test_task1.items()]\n",
    "max_len_test_task1 = max([len(x) for x in seq_x_test_task1])\n",
    "\n",
    "seq_x_china_task1 = [v[\"tokenized\"] for k,v in dict_china_task1.items()]\n",
    "max_len_china_task1 = max([len(x) for x in seq_x_china_task1])\n",
    "\n",
    "max_len_task1 = max(max_len_dev_task1, max_len_train_task1, max_len_test_task1, max_len_china_task1)\n",
    "seq_x_train_task1 = np.stack([[np.zeros(300)]*(max_len_task1-len(x))+x for x in seq_x_train_task1])\n",
    "seq_x_dev_task1 = np.stack([[np.zeros(300)]*(max_len_task1-len(x))+x for x in seq_x_dev_task1])\n",
    "seq_x_test_task1 = np.stack([[np.zeros(300)]*(max_len_task1-len(x))+x for x in seq_x_test_task1])\n",
    "seq_x_china_task1 = np.stack([[np.zeros(300)]*(max_len_task1-len(x))+x for x in seq_x_china_task1])\n",
    "\n",
    "print(max_len_task1)\n",
    "\n",
    "seq_x_train_task2 = [v[\"tokenized\"] for k,v in dict_train_task2.items()]\n",
    "seq_y_train_task2 = np.array([v[\"label\"] for k,v in dict_train_task2.items()])\n",
    "max_len_train_task2 = max([len(x) for x in seq_x_train_task2])\n",
    "\n",
    "seq_x_dev_task2 = [v[\"tokenized\"] for k,v in dict_dev_task2.items()]\n",
    "seq_y_dev_task2 = np.array([v[\"label\"] for k,v in dict_dev_task2.items()])\n",
    "max_len_dev_task2 = max([len(x) for x in seq_x_dev_task2])\n",
    "\n",
    "seq_x_test_task2 = [v[\"tokenized\"] for k,v in dict_test_task2.items()]\n",
    "max_len_test_task2 = max([len(x) for x in seq_x_test_task2])\n",
    "\n",
    "seq_x_china_task2 = [v[\"tokenized\"] for k,v in dict_china_task2.items()]\n",
    "max_len_china_task2 = max([len(x) for x in seq_x_china_task2])\n",
    "\n",
    "max_len_task2 = max(max_len_dev_task2, max_len_train_task2, max_len_test_task2, max_len_china_task2)\n",
    "seq_x_train_task2 = np.stack([[np.zeros(300)]*(max_len_task2-len(x))+x for x in seq_x_train_task2])\n",
    "seq_x_dev_task2 = np.stack([[np.zeros(300)]*(max_len_task2-len(x))+x for x in seq_x_dev_task2])\n",
    "seq_x_test_task2 = np.stack([[np.zeros(300)]*(max_len_task2-len(x))+x for x in seq_x_test_task2])\n",
    "seq_x_china_task2 = np.stack([[np.zeros(300)]*(max_len_task2-len(x))+x for x in seq_x_china_task2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del fasttext\n",
    "del df_train_task2\n",
    "del df_dev_task2\n",
    "del df_test_task2\n",
    "del df_train_task1\n",
    "del df_dev_task1\n",
    "del df_test_task1\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_task1 = seq_x_train_task1.shape[0]\n",
    "n_train_task2 = seq_x_train_task2.shape[0]\n",
    "batch_size = 128\n",
    "\n",
    "def generate_data(x_doc, y_doc, batch_size):\n",
    "    i_doc = 0\n",
    "    n_doc = x_doc.shape[0]\n",
    "    while True:  \n",
    "        j_doc = i_doc+batch_size\n",
    "        if j_doc > n_doc:\n",
    "            j_doc = n_doc\n",
    "        output = (x_doc[i_doc:j_doc,:], y_doc[i_doc:j_doc])\n",
    "        i_doc = j_doc\n",
    "        if i_doc == n_doc:\n",
    "            i_doc = 0\n",
    "        yield output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "seq_in = Input(shape=(None,300))\n",
    "\n",
    "lstm_layer = Bidirectional(CuDNNLSTM(10, return_sequences=False))\n",
    "doc_out = Dense(1, activation=\"sigmoid\")(GaussianDropout(0.4)(lstm_layer(GaussianDropout(0.5)(seq_in))))\n",
    "sent_out = Dense(1, activation=\"sigmoid\")(GaussianDropout(0.6)(lstm_layer(GaussianDropout(0.5)(seq_in))))\n",
    "\n",
    "rmsprop_1 = RMSprop(lr=0.005, rho=0.9, epsilon=None, decay=0.0)\n",
    "rmsprop_2 = RMSprop(lr=0.005, rho=0.9, epsilon=None, decay=0.0)\n",
    "\n",
    "doc_model = Model(inputs=seq_in,outputs=doc_out)\n",
    "doc_model.compile(rmsprop_1, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "print(doc_model.summary())\n",
    "\n",
    "sent_model = Model(inputs=seq_in,outputs=sent_out)\n",
    "sent_model.compile(rmsprop_2, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "print(sent_model.summary())\n",
    "\n",
    "gen_doc = generate_data(seq_x_train_task1, seq_y_train_task1, batch_size)\n",
    "gen_sen = generate_data(seq_x_train_task2, seq_y_train_task2, batch_size)\n",
    "\n",
    "d_train_metrics = []\n",
    "d_dev_metrics = []\n",
    "s_train_metrics = []\n",
    "s_dev_metrics = []\n",
    "\n",
    "for ii in range(100):\n",
    "    print(f\"========== EPOCH {ii} ==========\")\n",
    "    \n",
    "    (loss_d_train, acc_d_train) = doc_model.evaluate(seq_x_train_task1, seq_y_train_task1, batch_size=256, verbose=0)\n",
    "    (loss_d_dev, acc_d_dev) = doc_model.evaluate([np.stack(seq_x_dev_task1)], np.array(seq_y_dev_task1), batch_size=256, verbose=0)\n",
    "    (loss_s_train, acc_s_train) = sent_model.evaluate(seq_x_train_task2, seq_y_train_task2, batch_size=256, verbose=0)\n",
    "    (loss_s_dev, acc_s_dev) = sent_model.evaluate([np.stack(seq_x_dev_task2)], np.array(seq_y_dev_task2), batch_size=256, verbose=0)\n",
    "    print(f\"[{loss_d_train:.3f}\\t{loss_d_dev:.3f}]\\t[{acc_d_train:.3f}\\t{acc_d_dev:.3f}]\\t[{loss_s_train:.3f}\\t{loss_s_dev:.3f}]\\t[{acc_s_train:.3f}\\t{acc_s_dev:.3f}]\")\n",
    "\n",
    "    d_train_metrics.append((loss_d_train, acc_d_train))\n",
    "    d_dev_metrics.append((loss_d_dev, acc_d_dev))\n",
    "    s_train_metrics.append((loss_s_train, acc_s_train))\n",
    "    s_dev_metrics.append((loss_s_dev, acc_s_dev))\n",
    "    \n",
    "    for ee in range(20):\n",
    "        \n",
    "        batch = next(gen_doc)\n",
    "        doc_model.train_on_batch(batch[0],batch[1],class_weight=\"auto\")\n",
    "        batch = next(gen_sen)\n",
    "        sent_model.train_on_batch(batch[0],batch[1],class_weight=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Task 1 Dev results on Model 1:\")\n",
    "print(doc_model.evaluate([np.stack(seq_x_dev_task1)], np.array(seq_y_dev_task1)))\n",
    "print(f1_score(np.array(seq_y_dev_task1), np.round(doc_model.predict([np.stack(seq_x_dev_task1)]))))\n",
    "\n",
    "print(\"Task 1 Dev results on Model 2:\")\n",
    "print(sent_model.evaluate([np.stack(seq_x_dev_task1)], np.array(seq_y_dev_task1)))\n",
    "print(f1_score(np.array(seq_y_dev_task1), np.round(sent_model.predict([np.stack(seq_x_dev_task1)]))))\n",
    "\n",
    "print(\"Task 2 Dev results on Model 2:\")\n",
    "print(sent_model.evaluate([np.stack(seq_x_dev_task2)], np.array(seq_y_dev_task2)))\n",
    "print(f1_score(np.array(seq_y_dev_task2), np.round(sent_model.predict([np.stack(seq_x_dev_task2)]))))\n",
    "\n",
    "print(\"Task 2 Dev results on Model 1:\")\n",
    "print(doc_model.evaluate([np.stack(seq_x_dev_task2)], np.array(seq_y_dev_task2)))\n",
    "print(f1_score(np.array(seq_y_dev_task2), np.round(doc_model.predict([np.stack(seq_x_dev_task2)]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_train_preds = np.round(doc_model.predict([np.stack(seq_x_train_task1)]))\n",
    "task1_dev_preds = np.round(doc_model.predict([np.stack(seq_x_dev_task1)]))\n",
    "task2_train_preds = np.round(sent_model.predict(np.stack(seq_x_train_task2)))\n",
    "task2_dev_preds = np.round(sent_model.predict(np.stack(seq_x_dev_task2)))\n",
    "task2_train_preds_mod1 = np.round(doc_model.predict([np.stack(seq_x_train_task2)]))\n",
    "task2_dev_preds_mod1 = np.round(doc_model.predict(np.stack(seq_x_dev_task2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pal = [\"#FF0000\", \"#00A08A\", \"#F2AD00\", \"#F98400\", \"#5BBCD6\"]\n",
    "\n",
    "plt.figure(figsize=(3.5,3.5))\n",
    "plt.plot(range(100), np.array([a[0] for a in d_train_metrics]), c=pal[0], ls='-', label=\"Task 1 train\")\n",
    "plt.plot(range(100), np.array([a[0] for a in d_dev_metrics]), c=pal[0], ls='--', label=\"Task 1 dev\")\n",
    "plt.plot(range(100), np.array([a[0] for a in s_train_metrics]), c=pal[2], ls='-', label=\"Task 2 train\")\n",
    "plt.plot(range(100), np.array([a[0] for a in s_dev_metrics]), c=pal[2], ls='--', label=\"Task 2 dev\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.legend()\n",
    "# plt.savefig(\"final_task_12/task12_loss.pdf\", transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3.5,3.5))\n",
    "plt.plot(range(100), np.array([a[1] for a in d_train_metrics]), c=pal[0], ls='-', label=\"Task 1 train\")\n",
    "plt.plot(range(100), np.array([a[1] for a in d_dev_metrics]), c=pal[0], ls='--', label=\"Task 1 dev\")\n",
    "plt.plot(range(100), np.array([a[1] for a in s_train_metrics]), c=pal[2], ls='-', label=\"Task 2 train\")\n",
    "plt.plot(range(100), np.array([a[1] for a in s_dev_metrics]), c=pal[2], ls='--', label=\"Task 2 dev\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification Accuracy')\n",
    "plt.legend()\n",
    "# plt.savefig(\"final_task_12/task12_accuracy.pdf\", transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "#     classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "#     ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plt.figure(figsize=(3.5,3.5))\n",
    "plot_confusion_matrix(np.array(seq_y_dev_task1), task1_dev_preds, classes=[\"No Protest\",\"Protest\"], normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "# plt.savefig(\"final_task_12/task1_dev_confusion.pdf\",transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(3.5,3.5))\n",
    "plot_confusion_matrix(np.array(seq_y_dev_task2), task2_dev_preds, classes=[\"No Protest\",\"Protest\"], normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "# plt.savefig(\"final_task_12/task2_dev_confusion.pdf\",transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(3.5,3.5))\n",
    "plot_confusion_matrix(np.array(seq_y_dev_task2), task2_dev_preds_mod1, classes=[\"No Protest\",\"Protest\"], normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "# plt.savefig(\"final_task_12/task2_model1_dev_confusion.pdf\",transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"public_data/phase2/task1_test.data\") as f:\n",
    "    task1_test_ids = f.read().splitlines()\n",
    "with open(\"public_data/phase2/task2_test.data\") as f:\n",
    "    task2_test_ids = f.read().splitlines()\n",
    "with open(\"public_data/phase2/china_test_task1.data\") as f:\n",
    "    task1_china_ids = f.read().splitlines()\n",
    "with open(\"public_data/phase2/china_test_task2.data\") as f:\n",
    "    task2_china_ids = f.read().splitlines()\n",
    "with open(\"solutions/task1_train.data\") as f:\n",
    "    task1_train_ids = f.read().splitlines()\n",
    "with open(\"solutions/task2_train.data\") as f:\n",
    "    task2_train_ids = f.read().splitlines()\n",
    "with open(\"solutions/task1_dev.data\") as f:\n",
    "    task1_dev_ids = f.read().splitlines()\n",
    "with open(\"solutions/task2_dev.data\") as f:\n",
    "    task2_dev_ids = f.read().splitlines()\n",
    "    \n",
    "print(\"All Task1 IDs accounted for: \"+str(all([str(id_) in set(task1_test_ids) for id_ in dict_test_task1.keys()])))\n",
    "print(\"All Task2 IDs accounted for: \"+str(all([str(id_) in set([a.replace(\"_\",\"\",) for a in task2_test_ids]) for id_ in dict_test_task2.keys()])))\n",
    "print(\"All China1 IDs accounted for: \"+str(all([str(id_) in set(task1_china_ids) for id_ in dict_china_task1.keys()])))\n",
    "print(\"All China2 IDs accounted for: \"+str(all([str(id_) in set([a.replace(\"_\",\"\",) for a in task2_china_ids]) for id_ in dict_china_task2.keys()])))\n",
    "\n",
    "task1_train_results = list(zip(task1_train_ids, [int(a) for a in list(task1_train_preds[:,0])]))\n",
    "task2_train_results = list(zip(task2_train_ids, [int(a) for a in list(task2_train_preds[:,0])]))\n",
    "task1_dev_results = list(zip(task1_dev_ids, [int(a) for a in list(task1_dev_preds[:,0])]))\n",
    "task2_dev_results = list(zip(task2_dev_ids, [int(a) for a in list(task2_dev_preds[:,0])]))\n",
    "\n",
    "task1_train_results = [str(a) + \"\\t\" + str(b) for a,b in task1_train_results]\n",
    "task2_train_results = [str(a) + \"\\t\" + str(b) for a,b in task2_train_results]\n",
    "task1_dev_results = [str(a) + \"\\t\" + str(b) for a,b in task1_dev_results]\n",
    "task2_dev_results = [str(a) + \"\\t\" + str(b) for a,b in task2_dev_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"final_task_12/task1_train.predict\",\"w\") as f:\n",
    "#     for ll in task1_train_results:\n",
    "#         f.write(\"{}\\n\".format(ll))\n",
    "        \n",
    "# with open(\"final_task_12/task1_dev.predict\",\"w\") as f:\n",
    "#     for ll in task1_dev_results:\n",
    "#         f.write(\"{}\\n\".format(ll))\n",
    "        \n",
    "# with open(\"final_task_12/task2_train.predict\",\"w\") as f:\n",
    "#     for ll in task2_train_results:\n",
    "#         f.write(\"{}\\n\".format(ll))\n",
    "        \n",
    "# with open(\"final_task_12/task2_dev.predict\",\"w\") as f:\n",
    "#     for ll in task2_dev_results:\n",
    "#         f.write(\"{}\\n\".format(ll))\n",
    "        \n",
    "# doc_model.save('final_task_12/model_doc.h5') \n",
    "# sent_model.save('final_task_12/model_sent.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protest-news",
   "language": "python",
   "name": "protest-news"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
