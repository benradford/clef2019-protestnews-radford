{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "import os\n",
    "import gc \n",
    "import math\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\";  \n",
    "\n",
    "import glob\n",
    "import datetime\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import math\n",
    "from collections import Counter\n",
    "from ftfy import fix_text\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.wrappers import FastText \n",
    "\n",
    "from keras.preprocessing.sequence import make_sampling_table, skipgrams\n",
    "from keras.layers import Input, Dense, Embedding, Activation, Dot, Flatten, GRU, Dropout, LSTM, Bidirectional, GlobalMaxPooling1D, Concatenate, Lambda, CuDNNGRU, CuDNNLSTM, GaussianNoise, GaussianDropout, Conv1D, BatchNormalization, Softmax\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.activations import softmax\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "fasttext = FastText.load_fasttext_format(\"wiki.en.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "entity_map = {\"PERSON\":\"person\",\n",
    "              \"NORP\":\"nationality\",\n",
    "              \"FAC\":\"facility\",\n",
    "              \"ORG\":\"organization\",\n",
    "              \"GPE\":\"government\",\n",
    "              \"LOC\":\"location\",\n",
    "              \"PRODUCT\":\"product\",\n",
    "              \"EVENT\":\"event\",\n",
    "              \"WORK_OF_ART\":\"artwork\",\n",
    "              \"LAW\":\"law\",\n",
    "              \"LANGUAGE\":\"language\",\n",
    "              \"DATE\":\"date\",\n",
    "              \"TIME\":\"time\",\n",
    "              \"PERCENT\":\"percent\",\n",
    "              \"MONEY\":\"money\",\n",
    "              \"QUANTITY\":\"amount\",\n",
    "              \"ORDINAL\":\"first\",\n",
    "              \"CARDINAL\":\"number\"}\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub(' ', text)\n",
    "\n",
    "def strip_whitespace(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "def clean_text(text):\n",
    "    return(remove_punctuation(fix_text(strip_whitespace(remove_tags(text)))))\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return(re.sub(r\"[^A-Za-z0-9 ]\",\"\",text))\n",
    "\n",
    "def sub_entities(text, spacy_model):\n",
    "#     replacements = [(e.text,e.label_) for e in spacy_model(text).ents]\n",
    "#     replacements = sorted(replacements, key=lambda x: len(x[0]), reverse=True)\n",
    "#     replacements = [(a,entity_map[b]) for a,b in replacements]\n",
    "#     for k,v in replacements:\n",
    "#         text = re.sub(r\"\\b\"+re.escape(k)+r\"\\b\", v, text)\n",
    "# #     text = re.sub(r\"[^a-zA-Z0-9 ]\",\"\",text).lower()\n",
    "    return(text)\n",
    "\n",
    "def df_to_dic(df):\n",
    "    data_dic = {}\n",
    "    for item in df.iterrows():\n",
    "        data = item[1]\n",
    "        _id = data[\"id\"]\n",
    "        data_dic[_id] = {}\n",
    "        data_dic[_id][\"label\"] = data[\"label\"]\n",
    "        data_dic[_id][\"text\"] = data[\"text\"]\n",
    "        data_dic[_id][\"url\"] = data[\"url\"]\n",
    "    return data_dic\n",
    "\n",
    "def task3_listify(df,spacy_model):\n",
    "    return_list = []\n",
    "    maxn = max(df[\"sentence\"].tolist())\n",
    "    for ii in range(maxn+1):\n",
    "        subset = df.loc[df.sentence==ii]\n",
    "        sentence = subset[\"word\"].tolist()\n",
    "        y = subset[\"label\"].tolist()\n",
    "        y = [a if a is not None else \"O\" for a in y]\n",
    "        ent_sub = [((e.text+\" \").split(\" \"), e.label_) for e in spacy_model(' '.join(sentence)).ents]\n",
    "        ent_sub = [(a, [b]*len(a)) for a,b in ent_sub]\n",
    "        ent_sub = {k:v for k,v in zip([item for sublist in [a for a,b in ent_sub] for item in sublist],\n",
    "                                      [item for sublist in [b for a,b in ent_sub] for item in sublist])}\n",
    "        entities = [ent_sub[a] if a in ent_sub.keys() else \"O\" for a in sentence]\n",
    "        return_list.append({\"text\":[s for s in sentence], \"y\":y, \"entities\":entities})\n",
    "    return(return_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## News Data\n",
    "\n",
    "file_name_train_task1 = \"data/Document/train_filled.json\"\n",
    "file_name_dev_task1 = \"data/Document/dev_filled.json\"\n",
    "file_name_test_task1 = \"data/Document/test_filled.json\"\n",
    "file_name_china_task1 = \"data/Document/test_china_filled.json\"\n",
    "\n",
    "file_name_train_task2 = \"data/Sentence/train_filled.json\"\n",
    "file_name_dev_task2 = \"data/Sentence/dev_filled.json\"\n",
    "file_name_test_task2 = \"data/Sentence/test_filled.json\"\n",
    "file_name_china_task2 = \"data/Sentence/test_china_filled.json\"\n",
    "\n",
    "## JSON - to - DF\n",
    "\n",
    "df_train_task1 = pd.read_json(file_name_train_task1, orient=\"records\", lines = True)\n",
    "df_train_task1[\"text\"] = [sub_entities(clean_text(t), nlp) for t in df_train_task1.text]\n",
    "\n",
    "df_dev_task1 = pd.read_json(file_name_dev_task1, orient=\"records\", lines = True)\n",
    "df_dev_task1[\"text\"] = [sub_entities(clean_text(t), nlp) for t in df_dev_task1.text]\n",
    "\n",
    "df_test_task1 = pd.read_json(file_name_test_task1, orient=\"records\", lines = True)\n",
    "df_test_task1[\"text\"] = [sub_entities(clean_text(t), nlp) for t in df_test_task1.text]\n",
    "df_test_task1[\"label\"] = -1\n",
    "\n",
    "df_train_task2 = pd.read_json(file_name_train_task2, orient=\"records\", lines = True)\n",
    "df_train_task2[\"text\"] = [sub_entities(clean_text(t), nlp) for t in df_train_task2.sentence]\n",
    "\n",
    "df_dev_task2 = pd.read_json(file_name_dev_task2, orient=\"records\", lines = True)\n",
    "df_dev_task2[\"text\"] = [sub_entities(clean_text(t), nlp) for t in df_dev_task2.sentence]\n",
    "\n",
    "df_test_task2 = pd.read_json(file_name_test_task2, orient=\"records\", lines = True)\n",
    "df_test_task2[\"text\"] = [sub_entities(clean_text(t), nlp) for t in df_test_task2.sentence]\n",
    "df_test_task2[\"label\"] = -1\n",
    "\n",
    "df_china_task1 = pd.read_json(file_name_china_task1, orient=\"records\", lines = True)\n",
    "df_china_task1[\"text\"] = [sub_entities(clean_text(t), nlp) for t in df_china_task1.text]\n",
    "df_china_task1[\"label\"] = -1\n",
    "\n",
    "df_china_task2 = pd.read_json(file_name_china_task2, orient=\"records\", lines = True)\n",
    "df_china_task2[\"text\"] = [sub_entities(clean_text(t), nlp) for t in df_china_task2.sentence]\n",
    "df_china_task2[\"label\"] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_train_task3 = \"task3public9may/train.txt\"\n",
    "file_name_dev_task3 = \"task3public9may/dev.txt\"\n",
    "file_name_test_task3 = \"task3public9may/task3_test.data\"\n",
    "file_name_china_task3 = \"task3public9may/china_test.data\"\n",
    "\n",
    "with open(file_name_train_task3) as f:\n",
    "    file = f.read().splitlines()\n",
    "    df_train_task3 = pd.DataFrame([a.split(\"\\t\") for a in file], columns=[\"word\",\"label\"])\n",
    "    df_train_task3[\"word\"] = [a.strip() for a in df_train_task3[\"word\"].tolist()]\n",
    "    df_train_task3[\"label\"] = [str(a) if a!=\"\" else \"\" for a in df_train_task3[\"label\"].tolist()]\n",
    "\n",
    "with open(file_name_dev_task3) as f:\n",
    "    file = f.read().splitlines()\n",
    "    df_dev_task3 = pd.DataFrame([a.split(\"\\t\") for a in file], columns=[\"word\",\"label\"])\n",
    "    df_dev_task3[\"word\"] = [a.strip() for a in df_dev_task3[\"word\"].tolist()]\n",
    "    df_dev_task3[\"label\"] = [str(a) if a!=\"\" else \"\" for a in df_dev_task3[\"label\"].tolist()]\n",
    "#     \n",
    "with open(file_name_test_task3) as f:\n",
    "    df_test_task3 = pd.DataFrame([(a.strip(),-1) for a in f.readlines()], columns=[\"word\",\"label\"])\n",
    "with open(file_name_china_task3) as f:\n",
    "    df_china_task3 = pd.DataFrame([(a.strip(),-1) for a in f.readlines()], columns=[\"word\",\"label\"])\n",
    "    \n",
    "df_train_task3[\"sentence\"] = (df_train_task3[\"word\"]==\"SAMPLE_START\").cumsum() - 1\n",
    "df_dev_task3[\"sentence\"] = (df_dev_task3[\"word\"]==\"SAMPLE_START\").cumsum() - 1\n",
    "df_test_task3[\"sentence\"] = (df_test_task3[\"word\"]==\"SAMPLE_START\").cumsum() - 1 \n",
    "df_china_task3[\"sentence\"] = (df_china_task3[\"word\"]==\"SAMPLE_START\").cumsum() - 1\n",
    "\n",
    "task3_y_values = set([a for a in df_train_task3[\"label\"].tolist() + df_dev_task3[\"label\"].tolist() if a is not None])\n",
    "task3_y_fwd_dict = {k:v for v,k in enumerate(task3_y_values)}\n",
    "task3_y_rev_dict = {k:v for v,k in task3_y_fwd_dict.items()}\n",
    "\n",
    "train_task3 = task3_listify(df_train_task3,nlp)\n",
    "test_task3 = task3_listify(df_test_task3,nlp)\n",
    "dev_task3 = task3_listify(df_dev_task3,nlp)\n",
    "china_task3 = task3_listify(df_china_task3,nlp)\n",
    "\n",
    "task3_ent_values = set(item for sublist in \n",
    "                       [a['entities'] for a in train_task3] + \n",
    "                       [a['entities'] for a in test_task3] + \n",
    "                       [a['entities'] for a in dev_task3] + \n",
    "                       [a['entities'] for a in china_task3]\n",
    "                       for item in sublist)\n",
    "task3_ent_fwd_dict = {k:v for v,k in enumerate(task3_ent_values)}\n",
    "task3_ent_rev_dict = {k:v for v,k in task3_ent_fwd_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_china_task1.head())\n",
    "print(df_train_task3.head())\n",
    "print(dev_task3[0])\n",
    "print(task3_y_fwd_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_pass(text, model):\n",
    "    try:\n",
    "        return(model[text])\n",
    "    except:\n",
    "        pass\n",
    "def try_zeros(text, model, dim=300):\n",
    "    try:\n",
    "        return(model[text])\n",
    "    except:\n",
    "        return(np.zeros(dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataframe to Dictionary\n",
    "\n",
    "dict_train_task1 = df_to_dic(df_train_task1)\n",
    "dict_test_task1 = df_to_dic(df_test_task1)\n",
    "dict_dev_task1 = df_to_dic(df_dev_task1)\n",
    "dict_china_task1 = df_to_dic(df_china_task1)\n",
    "\n",
    "dict_train_task2 = df_to_dic(df_train_task2)\n",
    "dict_test_task2 = df_to_dic(df_test_task2)\n",
    "dict_dev_task2 = df_to_dic(df_dev_task2)\n",
    "dict_china_task2 = df_to_dic(df_china_task2)\n",
    "\n",
    "## Sub in Word Vectors\n",
    "\n",
    "for id_, dic in dict_train_task1.items():\n",
    "    dict_train_task1[id_][\"tokenized\"] = [try_pass(a,fasttext) for a in dic[\"text\"].split()]# if a in fasttext.vocab.keys()]\n",
    "    dict_train_task1[id_][\"tokenized\"] = [a for a in dict_train_task1[id_][\"tokenized\"] if a is not None]\n",
    "for id_, dic in dict_test_task1.items(): \n",
    "    dict_test_task1[id_][\"tokenized\"] = [try_pass(a,fasttext) for a in dic[\"text\"].split()]# if a in fasttext.vocab.keys()]\n",
    "    dict_test_task1[id_][\"tokenized\"] = [a for a in dict_test_task1[id_][\"tokenized\"] if a is not None]\n",
    "for id_, dic in dict_dev_task1.items():\n",
    "    dict_dev_task1[id_][\"tokenized\"] = [try_pass(a,fasttext) for a in dic[\"text\"].split()]# if a in fasttext.vocab.keys()]\n",
    "    dict_dev_task1[id_][\"tokenized\"] = [a for a in dict_dev_task1[id_][\"tokenized\"] if a is not None]\n",
    "for id_, dic in dict_china_task1.items():\n",
    "    dict_china_task1[id_][\"tokenized\"] = [try_pass(a,fasttext) for a in dic[\"text\"].split()]# if a in fasttext.vocab.keys()]\n",
    "    dict_china_task1[id_][\"tokenized\"] = [a for a in dict_china_task1[id_][\"tokenized\"] if a is not None]\n",
    "    \n",
    "for id_, dic in dict_train_task2.items():\n",
    "    dict_train_task2[id_][\"tokenized\"] = [try_pass(a,fasttext) for a in dic[\"text\"].split()]# if a in fasttext.vocab.keys()]\n",
    "    dict_train_task2[id_][\"tokenized\"] = [a for a in dict_train_task2[id_][\"tokenized\"] if a is not None]\n",
    "for id_, dic in dict_test_task2.items(): \n",
    "    dict_test_task2[id_][\"tokenized\"] = [try_pass(a,fasttext) for a in dic[\"text\"].split()]# if a in fasttext.vocab.keys()]\n",
    "    dict_test_task2[id_][\"tokenized\"] = [a for a in dict_test_task2[id_][\"tokenized\"] if a is not None]\n",
    "for id_, dic in dict_dev_task2.items():\n",
    "    dict_dev_task2[id_][\"tokenized\"] = [try_pass(a,fasttext) for a in dic[\"text\"].split()]# if a in fasttext.vocab.keys()]\n",
    "    dict_dev_task2[id_][\"tokenized\"] = [a for a in dict_dev_task2[id_][\"tokenized\"] if a is not None]\n",
    "for id_, dic in dict_china_task2.items():\n",
    "    dict_china_task2[id_][\"tokenized\"] = [try_pass(a,fasttext) for a in dic[\"text\"].split()]# if a in fasttext.vocab.keys()]\n",
    "    dict_china_task2[id_][\"tokenized\"] = [a for a in dict_china_task2[id_][\"tokenized\"] if a is not None]\n",
    "                                  \n",
    "train_task3 = [dict(a,**{\"tokenized\":[try_zeros(b,fasttext) for b in a[\"text\"]]}) for a in train_task3]\n",
    "test_task3 = [dict(a,**{\"tokenized\":[try_zeros(b,fasttext) for b in a[\"text\"]]}) for a in test_task3]\n",
    "dev_task3 = [dict(a,**{\"tokenized\":[try_zeros(b,fasttext) for b in a[\"text\"]]}) for a in dev_task3]\n",
    "china_task3 = [dict(a,**{\"tokenized\":[try_zeros(b,fasttext) for b in a[\"text\"]]}) for a in china_task3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_x_train_task1 = [v[\"tokenized\"] for k,v in dict_train_task1.items()]\n",
    "seq_y_train_task1 = np.array([v[\"label\"] for k,v in dict_train_task1.items()])\n",
    "max_len_train_task1 = max([len(x) for x in seq_x_train_task1])\n",
    "\n",
    "seq_x_dev_task1 = [v[\"tokenized\"] for k,v in dict_dev_task1.items()]\n",
    "seq_y_dev_task1 = np.array([v[\"label\"] for k,v in dict_dev_task1.items()])\n",
    "max_len_dev_task1 = max([len(x) for x in seq_x_dev_task1])\n",
    "\n",
    "seq_x_test_task1 = [v[\"tokenized\"] for k,v in dict_test_task1.items()]\n",
    "max_len_test_task1 = max([len(x) for x in seq_x_test_task1])\n",
    "\n",
    "seq_x_china_task1 = [v[\"tokenized\"] for k,v in dict_china_task1.items()]\n",
    "max_len_china_task1 = max([len(x) for x in seq_x_china_task1])\n",
    "\n",
    "max_len_task1 = max(max_len_dev_task1, max_len_train_task1, max_len_test_task1, max_len_china_task1)\n",
    "seq_x_train_task1 = np.stack([[np.zeros(300)]*(max_len_task1-len(x))+x for x in seq_x_train_task1])\n",
    "seq_x_dev_task1 = np.stack([[np.zeros(300)]*(max_len_task1-len(x))+x for x in seq_x_dev_task1])\n",
    "seq_x_test_task1 = np.stack([[np.zeros(300)]*(max_len_task1-len(x))+x for x in seq_x_test_task1])\n",
    "seq_x_china_task1 = np.stack([[np.zeros(300)]*(max_len_task1-len(x))+x for x in seq_x_china_task1])\n",
    "\n",
    "print(max_len_task1)\n",
    "\n",
    "seq_x_train_task2 = [v[\"tokenized\"] for k,v in dict_train_task2.items()]\n",
    "seq_y_train_task2 = np.array([v[\"label\"] for k,v in dict_train_task2.items()])\n",
    "max_len_train_task2 = max([len(x) for x in seq_x_train_task2])\n",
    "\n",
    "seq_x_dev_task2 = [v[\"tokenized\"] for k,v in dict_dev_task2.items()]\n",
    "seq_y_dev_task2 = np.array([v[\"label\"] for k,v in dict_dev_task2.items()])\n",
    "max_len_dev_task2 = max([len(x) for x in seq_x_dev_task2])\n",
    "\n",
    "seq_x_test_task2 = [v[\"tokenized\"] for k,v in dict_test_task2.items()]\n",
    "max_len_test_task2 = max([len(x) for x in seq_x_test_task2])\n",
    "\n",
    "seq_x_china_task2 = [v[\"tokenized\"] for k,v in dict_china_task2.items()]\n",
    "max_len_china_task2 = max([len(x) for x in seq_x_china_task2])\n",
    "\n",
    "max_len_task2 = max(max_len_dev_task2, max_len_train_task2, max_len_test_task2, max_len_china_task2)\n",
    "seq_x_train_task2 = np.stack([[np.zeros(300)]*(max_len_task2-len(x))+x for x in seq_x_train_task2])\n",
    "seq_x_dev_task2 = np.stack([[np.zeros(300)]*(max_len_task2-len(x))+x for x in seq_x_dev_task2])\n",
    "seq_x_test_task2 = np.stack([[np.zeros(300)]*(max_len_task2-len(x))+x for x in seq_x_test_task2])\n",
    "seq_x_china_task2 = np.stack([[np.zeros(300)]*(max_len_task2-len(x))+x for x in seq_x_china_task2])\n",
    "\n",
    "print(max_len_task2)\n",
    "\n",
    "##\n",
    "## Word-level classifier\n",
    "##\n",
    "seq_x_train_task3 = [v[\"tokenized\"] for v in train_task3]\n",
    "seq_x_dev_task3 = [v[\"tokenized\"] for v in dev_task3]\n",
    "seq_x_test_task3 = [v[\"tokenized\"] for v in test_task3]\n",
    "seq_x_china_task3 = [v[\"tokenized\"] for v in china_task3]\n",
    "seq_y_train_task3 = [v[\"y\"] for v in train_task3]\n",
    "seq_y_dev_task3 = [v[\"y\"] for v in dev_task3]\n",
    "\n",
    "seq_ent_train_task3 = [v[\"entities\"] for v in train_task3]\n",
    "seq_ent_dev_task3 = [v[\"entities\"] for v in dev_task3]\n",
    "seq_ent_test_task3 = [v[\"entities\"] for v in test_task3]\n",
    "seq_ent_china_task3 = [v[\"entities\"] for v in china_task3]\n",
    "\n",
    "max_len_task3 = max([len(a) for a in seq_x_train_task3 + seq_x_dev_task3 + seq_x_test_task3 + seq_x_china_task3])\n",
    "\n",
    "seq_x_train_task3 = np.stack([[np.zeros(300)]*(max_len_task3-len(x))+x for x in seq_x_train_task3])\n",
    "seq_x_dev_task3 = np.stack([[np.zeros(300)]*(max_len_task3-len(x))+x for x in seq_x_dev_task3])\n",
    "seq_x_test_task3 = np.stack([[np.zeros(300)]*(max_len_task3-len(x))+x for x in seq_x_test_task3])\n",
    "seq_x_china_task3 = np.stack([[np.zeros(300)]*(max_len_task3-len(x))+x for x in seq_x_china_task3])\n",
    "seq_y_train_task3 = np.stack(np.array([task3_y_fwd_dict[a] for a in [\"O\"]*(max_len_task3-len(x))+x]) for x in seq_y_train_task3)\n",
    "seq_y_dev_task3 = np.stack(np.array([task3_y_fwd_dict[a] for a in [\"O\"]*(max_len_task3-len(x))+x]) for x in seq_y_dev_task3)\n",
    "\n",
    "seq_ent_train_task3 = np.stack(np.array([task3_ent_fwd_dict[a] for a in [\"O\"]*(max_len_task3-len(x))+x]) for x in seq_ent_train_task3)\n",
    "seq_ent_dev_task3 = np.stack(np.array([task3_ent_fwd_dict[a] for a in [\"O\"]*(max_len_task3-len(x))+x]) for x in seq_ent_dev_task3)\n",
    "seq_ent_test_task3 = np.stack(np.array([task3_ent_fwd_dict[a] for a in [\"O\"]*(max_len_task3-len(x))+x]) for x in seq_ent_test_task3)\n",
    "seq_ent_china_task3 = np.stack(np.array([task3_ent_fwd_dict[a] for a in [\"O\"]*(max_len_task3-len(x))+x]) for x in seq_ent_china_task3)\n",
    "\n",
    "print(max_len_task3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_y_train_task3 = np.stack([to_categorical(a, len(task3_y_fwd_dict)) for a in seq_y_train_task3])\n",
    "seq_y_dev_task3 = np.stack([to_categorical(a, len(task3_y_fwd_dict)) for a in seq_y_dev_task3])\n",
    "print(seq_y_train_task3.shape)\n",
    "print(seq_y_dev_task3.shape)\n",
    "print(seq_ent_train_task3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del fasttext\n",
    "del df_train_task2\n",
    "del df_dev_task2\n",
    "del df_test_task2\n",
    "del df_train_task1\n",
    "del df_dev_task1\n",
    "del df_test_task1\n",
    "del df_train_task3\n",
    "del df_test_task3\n",
    "del df_dev_task3\n",
    "del df_china_task3\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_task1 = seq_x_train_task1.shape[0]\n",
    "n_train_task2 = seq_x_train_task2.shape[0]\n",
    "batch_size = 128\n",
    "\n",
    "def generate_data(x_doc, y_doc, batch_size):\n",
    "    i_doc = 0\n",
    "    n_doc = x_doc.shape[0]\n",
    "    while True:  \n",
    "        j_doc = i_doc+batch_size\n",
    "        if j_doc > n_doc:\n",
    "            j_doc = n_doc\n",
    "        output = (x_doc[i_doc:j_doc,:], y_doc[i_doc:j_doc])\n",
    "        i_doc = j_doc\n",
    "        if i_doc == n_doc:\n",
    "            i_doc = 0\n",
    "        yield output\n",
    "        \n",
    "def generate_xxy(x1_doc, x2_doc, y_doc, batch_size):\n",
    "    i_doc = 0\n",
    "    n_doc = x1_doc.shape[0]\n",
    "    while True:\n",
    "        j_doc = i_doc+batch_size\n",
    "        if j_doc > n_doc:\n",
    "            j_doc = n_doc\n",
    "        output = ([x1_doc[i_doc:j_doc,:], x2_doc[i_doc:j_doc,:]], y_doc[i_doc:j_doc])\n",
    "        i_doc = j_doc\n",
    "        if i_doc == n_doc:\n",
    "            i_doc = 0\n",
    "        yield output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seq_in = Input(shape=(None,300))\n",
    "\n",
    "ent_in = Input(shape=(None,))\n",
    "ent_emb = Embedding(input_dim=len(task3_ent_fwd_dict), output_dim=5)\n",
    "\n",
    "gru_layer = Bidirectional(CuDNNGRU(20, return_sequences=True))\n",
    "\n",
    "doc_out = Dense(1,activation=\"sigmoid\")(\n",
    "    GaussianDropout(rate=0.25)(\n",
    "        GlobalMaxPooling1D()(\n",
    "            gru_layer(GaussianDropout(rate=0.25)(seq_in)))))\n",
    "\n",
    "sent_out = Dense(1,activation=\"sigmoid\")(\n",
    "    GaussianDropout(rate=0.25)(\n",
    "        GlobalMaxPooling1D()(\n",
    "            gru_layer(GaussianDropout(rate=0.25)(seq_in)))))\n",
    "\n",
    "word_out = Softmax(axis=2)(\n",
    "    CuDNNGRU(len(task3_y_fwd_dict), return_sequences=True)(\n",
    "        GaussianDropout(rate=0.25)(\n",
    "    Activation(\"tanh\")(\n",
    "    Bidirectional(CuDNNGRU(len(task3_y_fwd_dict)*2, return_sequences=True))(\n",
    "        GaussianDropout(rate=0.25)(\n",
    "            Concatenate(axis=2)(\n",
    "                [seq_in, \n",
    "                 gru_layer(GaussianDropout(rate=0.25)(seq_in)), \n",
    "                 ent_emb(ent_in)])))))))\n",
    "\n",
    "rmsprop_1 = RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.0)\n",
    "rmsprop_2 = RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.0)\n",
    "rmsprop_3 = RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.0)\n",
    "\n",
    "doc_model = Model(inputs=seq_in,outputs=doc_out)\n",
    "doc_model.compile(rmsprop_1, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "print(doc_model.summary())\n",
    "\n",
    "sent_model = Model(inputs=seq_in,outputs=[sent_out])\n",
    "sent_model.compile(rmsprop_2, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "print(sent_model.summary())\n",
    "\n",
    "word_model = Model(inputs=[seq_in,ent_in],outputs=[word_out])\n",
    "word_model.compile(rmsprop_3, loss=[\"categorical_crossentropy\"], metrics=[\"categorical_accuracy\"])\n",
    "print(word_model.summary())\n",
    "\n",
    "gen_doc = generate_data(seq_x_train_task1, seq_y_train_task1, batch_size)\n",
    "gen_sen = generate_data(seq_x_train_task2, seq_y_train_task2, batch_size)\n",
    "gen_wor = generate_xxy(seq_x_train_task3, seq_ent_train_task3, seq_y_train_task3, batch_size)\n",
    "\n",
    "d_train_metrics = []\n",
    "d_dev_metrics = []\n",
    "s_train_metrics = []\n",
    "s_dev_metrics = []\n",
    "w_train_metrics = []\n",
    "w_dev_metrics = []\n",
    "\n",
    "for ii in range(100):\n",
    "    (loss_d_train, acc_d_train) = doc_model.evaluate(seq_x_train_task1, seq_y_train_task1, batch_size=256, verbose=0)\n",
    "    (loss_d_dev, acc_d_dev) = doc_model.evaluate([np.stack(seq_x_dev_task1)], np.array(seq_y_dev_task1), batch_size=256, verbose=0)\n",
    "    (loss_s_train, acc_s_train) = sent_model.evaluate(seq_x_train_task2, [seq_y_train_task2], batch_size=256, verbose=0)\n",
    "    (loss_s_dev, acc_s_dev) = sent_model.evaluate([np.stack(seq_x_dev_task2)], [np.array(seq_y_dev_task2)], batch_size=256, verbose=0)\n",
    "    (loss_w_train, acc_w_train_w) = word_model.evaluate([seq_x_train_task3, seq_ent_train_task3], [np.stack(seq_y_train_task3)], batch_size=256, verbose=0)\n",
    "    (loss_w_dev, acc_w_dev_w) = word_model.evaluate([np.stack(seq_x_dev_task3), seq_ent_dev_task3], [np.stack(seq_y_dev_task3)], batch_size=256, verbose=0)\n",
    "    print(f\"{ii:2.0f}:\\t[{loss_d_train:.3f}\\t{loss_d_dev:.3f}]\\t[{acc_d_train:.3f}\\t{acc_d_dev:.3f}]\\t[{loss_s_train:.3f}\\t{loss_s_dev:.3f}]\\t[{acc_s_train:.3f}\\t{acc_s_dev:.3f}]\\t[{loss_w_train:.3f}\\t{loss_w_dev:.3f}]\\t[{acc_w_train_w:.3f}\\t{acc_w_dev_w:.3f}]\")\n",
    "    \n",
    "    d_train_metrics.append((loss_d_train, acc_d_train))\n",
    "    d_dev_metrics.append((loss_d_dev, acc_d_dev))\n",
    "    s_train_metrics.append((loss_s_train, acc_s_train))\n",
    "    s_dev_metrics.append((loss_s_dev, acc_s_dev))\n",
    "    w_train_metrics.append((loss_w_train, acc_w_train_w))\n",
    "    w_dev_metrics.append((loss_w_dev, acc_w_dev_w))\n",
    "    \n",
    "    for ee in range(20):\n",
    "        batch = next(gen_doc)\n",
    "        doc_model.train_on_batch(batch[0],batch[1],class_weight=\"auto\")\n",
    "        batch = next(gen_sen)\n",
    "        sent_model.train_on_batch(batch[0],[batch[1]],class_weight=[\"auto\",\"auto\"])\n",
    "        batch = next(gen_wor)\n",
    "        word_model.train_on_batch(batch[0],[batch[1]],class_weight=[\"auto\"])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pal = [\"#FF0000\", \"#00A08A\", \"#F2AD00\", \"#F98400\", \"#5BBCD6\"]\n",
    "\n",
    "plt.figure(figsize=(3.5,3.5))\n",
    "plt.plot(range(100), np.array([a[0] for a in d_train_metrics]), c=pal[0], ls='-', label=\"Task 1 train\")\n",
    "plt.plot(range(100), np.array([a[0] for a in d_dev_metrics]), c=pal[0], ls='--', label=\"Task 1 dev\")\n",
    "plt.plot(range(100), np.array([a[0] for a in s_train_metrics]), c=pal[2], ls='-', label=\"Task 2 train\")\n",
    "plt.plot(range(100), np.array([a[0] for a in s_dev_metrics]), c=pal[2], ls='--', label=\"Task 2 dev\")\n",
    "plt.plot(range(100), np.array([a[0] for a in w_train_metrics]), c=pal[4], ls='-', label=\"Task 3 train\")\n",
    "plt.plot(range(100), np.array([a[0] for a in w_dev_metrics]), c=pal[4], ls='--', label=\"Task 3 dev\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.legend()\n",
    "#plt.savefig(\"final_task_3/task3_loss.pdf\", transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3.5,3.5))\n",
    "plt.plot(range(100), np.array([a[1] for a in d_train_metrics]), c=pal[0], ls='-', label=\"Task 1 train\")\n",
    "plt.plot(range(100), np.array([a[1] for a in d_dev_metrics]), c=pal[0], ls='--', label=\"Task 1 dev\")\n",
    "plt.plot(range(100), np.array([a[1] for a in s_train_metrics]), c=pal[2], ls='-', label=\"Task 2 train\")\n",
    "plt.plot(range(100), np.array([a[1] for a in s_dev_metrics]), c=pal[2], ls='--', label=\"Task 2 dev\")\n",
    "plt.plot(range(100), np.array([a[1] for a in w_train_metrics]), c=pal[4], ls='-', label=\"Task 3 train\")\n",
    "plt.plot(range(100), np.array([a[1] for a in w_dev_metrics]), c=pal[4], ls='--', label=\"Task 3 dev\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification Accuracy')\n",
    "plt.legend()\n",
    "#plt.savefig(\"final_task_3/task3_accuracy.pdf\", transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(a,b) for a,b in zip([task3_y_rev_dict[a] for a in np.argmax(word_model.predict([np.stack(seq_x_dev_task3),seq_ent_dev_task3]), axis=2)[1,:]],\n",
    "    [\"\"]*(max_len_task3-len(dev_task3[1][\"text\"])) + dev_task3[1][\"text\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "print(doc_model.evaluate([np.stack(seq_x_dev_task1)], np.array(seq_y_dev_task1)))\n",
    "print(f1_score(np.array(seq_y_dev_task1), np.round(doc_model.predict([np.stack(seq_x_dev_task1)]))))\n",
    "\n",
    "print(sent_model.evaluate([np.stack(seq_x_dev_task2)], np.array(seq_y_dev_task2)))\n",
    "print(f1_score(np.array(seq_y_dev_task2), np.round(sent_model.predict(np.stack(seq_x_dev_task2)))))\n",
    "\n",
    "print(word_model.evaluate([np.stack(seq_x_dev_task3), seq_ent_dev_task3], np.array(seq_y_dev_task3)))\n",
    "print(f1_score(np.argmax(np.array(seq_y_dev_task3),axis=2).flatten(), np.argmax(word_model.predict([np.stack(seq_x_dev_task3), seq_ent_dev_task3]), axis=2).flatten(), average=\"macro\"))\n",
    "print(precision_recall_fscore_support(np.argmax(np.array(seq_y_dev_task3),axis=2).flatten(), np.argmax(word_model.predict([np.stack(seq_x_dev_task3), seq_ent_dev_task3]), axis=2).flatten(), average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_train_preds = np.round(doc_model.predict([np.stack(seq_x_train_task1)]))\n",
    "task1_dev_preds = np.round(doc_model.predict([np.stack(seq_x_dev_task1)]))\n",
    "task2_train_preds = np.round(sent_model.predict(np.stack(seq_x_train_task2)))\n",
    "task2_dev_preds = np.round(sent_model.predict(np.stack(seq_x_dev_task2)))\n",
    "\n",
    "task3_train_preds = np.argmax(word_model.predict([np.stack(seq_x_train_task3), seq_ent_train_task3]), axis=2).tolist()\n",
    "task3_dev_preds = np.argmax(word_model.predict([np.stack(seq_x_dev_task3), seq_ent_dev_task3]), axis=2).tolist()\n",
    "task3_preds = np.argmax(word_model.predict([np.stack(seq_x_test_task3), seq_ent_test_task3]), axis=2).tolist()\n",
    "china_task3_preds = np.argmax(word_model.predict([np.stack(seq_x_china_task3), seq_ent_china_task3]), axis=2).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(np.array(seq_y_dev_task3),axis=2).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "#     classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "#     ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plot_confusion_matrix(np.argmax(np.array(seq_y_dev_task3),axis=2).flatten(), [item for sublist in task3_dev_preds for item in sublist], classes=[task3_y_rev_dict[a] for a in range(len(task3_y_rev_dict))], normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "#plt.savefig(\"final_task_3/task3_dev_confusion.pdf\",transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task3_train_preds = zip([a[\"text\"] for a in train_task3], task3_train_preds)\n",
    "task3_dev_preds = zip([a[\"text\"] for a in dev_task3], task3_dev_preds)\n",
    "\n",
    "task3_train_preds = [list(zip([\"REMOVE\"]*(max_len_task3-len(a)) + a, b)) for a,b in task3_train_preds]\n",
    "task3_dev_preds = [list(zip([\"REMOVE\"]*(max_len_task3-len(a)) + a, b)) for a,b in task3_dev_preds]\n",
    "\n",
    "task3_train_preds = [[(b[0],task3_y_rev_dict[b[1]]) for b in a if b[0]!=\"REMOVE\"] for a in task3_train_preds]\n",
    "task3_dev_preds = [[(b[0],task3_y_rev_dict[b[1]]) for b in a if b[0]!=\"REMOVE\"] for a in task3_dev_preds]\n",
    "\n",
    "###\n",
    "\n",
    "task3_preds = zip([a[\"text\"] for a in test_task3], task3_preds)\n",
    "china_task3_preds = zip([a[\"text\"] for a in china_task3], china_task3_preds)\n",
    "\n",
    "task3_preds = [list(zip([\"REMOVE\"]*(max_len_task3-len(a)) + a, b)) for a,b in task3_preds]\n",
    "china_task3_preds = [list(zip([\"REMOVE\"]*(max_len_task3-len(a)) + a, b)) for a,b in china_task3_preds]\n",
    "\n",
    "task3_preds = [[(b[0],task3_y_rev_dict[b[1]]) for b in a if b[0]!=\"REMOVE\"] for a in task3_preds]\n",
    "china_task3_preds = [[(b[0],task3_y_rev_dict[b[1]]) for b in a if b[0]!=\"REMOVE\"] for a in china_task3_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"public_data/phase1/task1_dev_filled.data\") as f:\n",
    "    task1_dev_ids = f.read().splitlines()\n",
    "with open(\"public_data/phase1/task1_train_filled.data\") as f:\n",
    "    task1_train_ids = f.read().splitlines()\n",
    "with open(\"public_data/phase1/task2_dev_filled.data\") as f:\n",
    "    task2_dev_ids = f.read().splitlines()\n",
    "with open(\"public_data/phase1/task2_train_filled.data\") as f:\n",
    "    task2_train_ids = f.read().splitlines()\n",
    "with open(\"task3public9may/dev.txt\") as f:\n",
    "    task3_dev_ids = [a.split(\"\\t\")[0] for a in f.read().splitlines()]\n",
    "with open(\"task3public9may/train.txt\") as f:\n",
    "    task3_train_ids = [a.split(\"\\t\")[0] for a in f.read().splitlines()]\n",
    "with open(\"task3public9may/task3_test.data\") as f:\n",
    "    task3_test_ids = f.read().splitlines()\n",
    "with open(\"task3public9may/china_test.data\") as f:\n",
    "    task3_china_ids = f.read().splitlines()\n",
    "    \n",
    "for kk, vv in dict_test_task1.items():\n",
    "    doc_vec = vv[\"tokenized\"]\n",
    "    doc_vec = [np.zeros(300)]*(max_len_task1 - len(doc_vec)) + doc_vec\n",
    "    dict_test_task1[kk][\"doc_vec\"] = doc_vec\n",
    "    \n",
    "for kk, vv in dict_test_task2.items():\n",
    "    doc_vec = vv[\"tokenized\"]\n",
    "    doc_vec = [np.zeros(300)]*(max_len_task2 - len(doc_vec)) + doc_vec\n",
    "    dict_test_task2[kk][\"doc_vec\"] = doc_vec\n",
    "    \n",
    "for kk, vv in dict_china_task1.items():\n",
    "    doc_vec = vv[\"tokenized\"]\n",
    "    doc_vec = [np.zeros(300)]*(max_len_task1 - len(doc_vec)) + doc_vec\n",
    "    dict_china_task1[kk][\"doc_vec\"] = doc_vec\n",
    "    \n",
    "for kk, vv in dict_china_task2.items():\n",
    "    doc_vec = vv[\"tokenized\"]\n",
    "    doc_vec = [np.zeros(300)]*(max_len_task2 - len(doc_vec)) + doc_vec\n",
    "    dict_china_task2[kk][\"doc_vec\"] = doc_vec\n",
    "\n",
    "task3_train_pred_tokens = [item[1] for sublist in task3_train_preds for item in sublist]\n",
    "task3_dev_pred_tokens = [item[1] for sublist in task3_dev_preds for item in sublist]\n",
    "task3_pred_tokens = [item[1] for sublist in task3_preds for item in sublist]\n",
    "task3_china_pred_tokens = [item[1] for sublist in china_task3_preds for item in sublist]\n",
    "\n",
    "def fix_BI(sequence):\n",
    "    new_seq = []\n",
    "    previous_token = \"O\"\n",
    "    for token in sequence:\n",
    "        if token == \"O\" or token == \"JUNK\":\n",
    "            new_seq.append(\"O\")\n",
    "        elif token == \"None\" or token == \"\":\n",
    "            new_seq.append(\"\")\n",
    "        else:\n",
    "            new_seq.append(token)\n",
    "#         else:\n",
    "#             if token == previous_token:\n",
    "#                 new_seq.append(\"I\"+token)\n",
    "#             else:\n",
    "#                 new_seq.append(\"B\"+token)\n",
    "#         previous_token = token\n",
    "#     new_seq = sequence\n",
    "    return(new_seq)\n",
    "     \n",
    "task3_train_ids = list(zip(task3_train_ids, fix_BI(task3_train_pred_tokens)))\n",
    "task3_dev_ids = list(zip(task3_dev_ids, fix_BI(task3_dev_pred_tokens)))\n",
    "task3_train_ids = [a + \"\\t\" + b if a!='' else '' for a, b in task3_train_ids]\n",
    "task3_dev_ids = [a + \"\\t\" + b if a!='' else '' for a, b in task3_dev_ids]\n",
    "\n",
    "task3_test_ids = list(zip(task3_test_ids, fix_BI(task3_pred_tokens)))\n",
    "task3_china_ids = list(zip(task3_china_ids, fix_BI(task3_china_pred_tokens)))\n",
    "task3_test_ids = [a + \"\\t\" + b if a!='' else '' for a, b in task3_test_ids]\n",
    "task3_china_ids = [a + \"\\t\" + b if a!='' else '' for a, b in task3_china_ids]\n",
    "    \n",
    "task1_train_results = list(zip(task1_train_ids, [int(a) for a in list(task1_train_preds[:,0])]))\n",
    "task2_train_results = list(zip(task2_train_ids, [int(a) for a in list(task2_train_preds[:,0])]))\n",
    "task1_dev_results = list(zip(task1_dev_ids, [int(a) for a in list(task1_dev_preds[:,0])]))\n",
    "task2_dev_results = list(zip(task2_dev_ids, [int(a) for a in list(task2_dev_preds[:,0])]))\n",
    "\n",
    "task1_train_results = [str(a) + \"\\t\" + str(b) for a,b in task1_train_results]\n",
    "task2_train_results = [str(a) + \"\\t\" + str(b) for a,b in task2_train_results]\n",
    "task1_dev_results = [str(a) + \"\\t\" + str(b) for a,b in task1_dev_results]\n",
    "task2_dev_results = [str(a) + \"\\t\" + str(b) for a,b in task2_dev_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"final_task_3/task3_train.predict\",\"w\") as f:\n",
    "#     for ll in task3_train_ids:\n",
    "#         f.write(\"{}\\n\".format(ll))\n",
    "        \n",
    "# with open(\"final_task_3/task3_dev.predict\",\"w\") as f:\n",
    "#     for ll in task3_dev_ids:\n",
    "#         f.write(\"{}\\n\".format(ll))\n",
    "        \n",
    "# with open(\"final_task_3/task1_train.predict\",\"w\") as f:\n",
    "#     for ll in task1_train_results:\n",
    "#         f.write(\"{}\\n\".format(ll))\n",
    "        \n",
    "# with open(\"final_task_3/task1_dev.predict\",\"w\") as f:\n",
    "#     for ll in task1_dev_results:\n",
    "#         f.write(\"{}\\n\".format(ll))\n",
    "        \n",
    "# with open(\"final_task_3/task2_train.predict\",\"w\") as f:\n",
    "#     for ll in task2_train_results:\n",
    "#         f.write(\"{}\\n\".format(ll))\n",
    "        \n",
    "# with open(\"final_task_3/task2_dev.predict\",\"w\") as f:\n",
    "#     for ll in task2_dev_results:\n",
    "#         f.write(\"{}\\n\".format(ll))\n",
    "        \n",
    "# doc_model.save('final_task_3/model_doc.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "# sent_model.save('final_task_3/model_sent.h5')\n",
    "# word_model.save('final_task_3/model_word.h5')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protest-news",
   "language": "python",
   "name": "protest-news"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
